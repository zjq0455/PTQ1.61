{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EWOtpjJMDmYE"
   },
   "source": [
    "# Reproduce Llama2-13b on a single A100 40GB"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This file including four section:\n",
    "- [(Optional) Train the quantization parameters of Llama2-13B by yourself.](#reproduce-llama2-13b-on-a-single-a100-40gb)\n",
    "- [Download our prebuilt quantized model.](#download-the-prebuilt-quantized-model)\n",
    "- [Reproduce Perplexity!](#reproduce-result)\n",
    "- [Reproduce ZeroShot](#reproduce-zeroshotusing-lm-evaluation-harness)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (Optional) Train the quantization parameters of Llama-2-13b-hf by yourself.\n",
    "\n",
    "This section provids how to train the quantization parameters of Llama-2-13b by yourself. You can skip this section because we have provided the pre-built quantized models in [Download the pre-quantized models](#download-the-pre-quantized-models)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CUDA_VISIBLE_DEVICES=5 python main.py --model /PATH/TO/MODEL --epochs 20 --output_dir ./log/llama2-13b --eval_ppl --wbits 4 --abits 16 --quant_type mix --lwc \\\n",
    "--ckpt_path /PATH/TO/CKPT \\\n",
    "--percent 0.2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download the prebuilt quantized model:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FwsWd1WbDmYE"
   },
   "source": [
    "We have provide the prebuilt quantized model on Huggingface. In order to download the large weights, we'll have to use `git lfs`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "V0GjINnMDmYF"
   },
   "outputs": [],
   "source": [
    "!conda install git git-lfs\n",
    "!git lfs install"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FSAe7Ew_DmYF"
   },
   "outputs": [],
   "source": [
    "!mkdir -p pre_quantized_models/\n",
    "\n",
    "!git clone https://huggingface.co/ptq161/llama2-13b ./pre_quantized_models/llama2-13b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "76Ru5__tDmYF"
   },
   "source": [
    "## Reproduce Result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Constraint in one GPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "torch.cuda.set_device(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Import necessary packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/anaconda3/envs/omniquant/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/root/anaconda3/envs/omniquant/lib/python3.10/site-packages/transformers/utils/generic.py:441: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
      "  _torch_pytree._register_pytree_node(\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer, AutoConfig\n",
    "from datautils import get_loaders\n",
    "from tqdm import tqdm\n",
    "from torch import nn\n",
    "import logging\n",
    "import gc   \n",
    "import time\n",
    "torch.backends.cuda.matmul.allow_tf32 = True\n",
    "logger = logging.getLogger(__name__)\n",
    "def get_model(model_path):\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        model_path, device_map=\"cpu\", torch_dtype=torch.float16\n",
    "    )\n",
    "    for n,p in model.named_parameters():\n",
    "        p.requires_grad = False\n",
    "    return model\n",
    "def evaluate(model, tokenizer_path, logger):\n",
    "    results = {}\n",
    "    device = model.device\n",
    "    seqlen = 2048\n",
    "    seed = 42\n",
    "    # for dataset in [\"wikitext2\", \"ptb\", \"c4\",\"ptb-new\",'c4-new']:\n",
    "    for dataset in [\"wikitext2\", \"c4\"]:\n",
    "        dataloader, testloader = get_loaders(\n",
    "            dataset,\n",
    "            seed=seed,\n",
    "            model=tokenizer_path,\n",
    "            seqlen=seqlen,\n",
    "        )\n",
    "        if \"c4\" in dataset:\n",
    "            testenc = testloader\n",
    "        else:\n",
    "            testenc = testloader.input_ids\n",
    "\n",
    "        nsamples = testenc.numel() // seqlen\n",
    "        use_cache = model.config.use_cache\n",
    "        model.config.use_cache = False\n",
    "        model.eval()\n",
    "        nlls = []\n",
    "        for i in tqdm(range(nsamples)):\n",
    "            batch = testenc[:, (i * seqlen) : ((i + 1) * seqlen)].to(device)\n",
    "            outputs = model.model(batch)\n",
    "            logits = outputs[0]\n",
    "            logits = model.lm_head(logits)\n",
    "            shift_logits = logits[:, :-1, :]\n",
    "            shift_labels = testenc[:, (i * seqlen) : ((i + 1) * seqlen)][\n",
    "                :, 1:\n",
    "            ].to(model.lm_head.weight.device)\n",
    "            loss_fct = nn.CrossEntropyLoss()\n",
    "            loss = loss_fct(\n",
    "                shift_logits.view(-1, shift_logits.size(-1)),\n",
    "                shift_labels.view(-1),\n",
    "            )\n",
    "            neg_log_likelihood = loss.float() * seqlen\n",
    "            nlls.append(neg_log_likelihood)\n",
    "\n",
    "        ppl = torch.exp(torch.stack(nlls).sum() / (nsamples * seqlen))\n",
    "        logger.info(f'{dataset} : {ppl.item()}')\n",
    "        model.config.use_cache = use_cache\n",
    "        results[dataset] = ppl.item()\n",
    "        print(\"dataset:\", ppl.item())\n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reproduce perplexity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/anaconda3/envs/omniquant/lib/python3.10/site-packages/transformers/utils/generic.py:309: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
      "  _torch_pytree._register_pytree_node(\n",
      "/root/anaconda3/envs/omniquant/lib/python3.10/site-packages/transformers/utils/generic.py:309: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
      "  _torch_pytree._register_pytree_node(\n",
      "Loading checkpoint shards: 100%|██████████| 6/6 [00:00<00:00,  9.00it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "get_wikitext2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 166/166 [01:11<00:00,  2.34it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataset: 9.665534019470215\n",
      "get_c4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/anaconda3/envs/omniquant/lib/python3.10/site-packages/datasets/table.py:1421: FutureWarning: promote has been superseded by promote_options='default'.\n",
      "  table = cls._concat_blocks(blocks, axis=0)\n",
      "100%|██████████| 256/256 [01:49<00:00,  2.33it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataset: 13.457582473754883\n",
      "perplexity result:\n",
      "wikitext2 9.665534019470215\n",
      "c4 13.457582473754883\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "model_path = \"/share/tmp/llama2-13b-mix-ptq2\"\n",
    "model = get_model(model_path)\n",
    "model.eval()\n",
    "device=torch.device(\"cuda:0\")\n",
    "model.to(device)\n",
    "results = evaluate(model, model_path, logger)\n",
    "print('perplexity result:')\n",
    "for k,v in results.items():\n",
    "    print(k, v)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reproduce Zeroshot(Using lm-evaluation-harness)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TASK=\"hellaswag,winogrande,race,piqa,mmlu,hellaswag,arc_easy,arc_challenge,lambada,ceval-valid\"\n",
    "MODEL_PATH=\"/PATH/TO/MODEL\"\n",
    "CUDA_VISIBLE_DEVICES=0 lm_eval --model hf \\\n",
    "    --model_args pretrained=$MODEL_PATH \\\n",
    "    --tasks $TASK \\\n",
    "    --device cuda:0 \\\n",
    "    --batch_size 4 \\\n",
    "    --output ./results/ptq161/$MODEL_PATH\n",
    "\n",
    "# Follow lm-eval-harness to install the environment."
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "omniquant",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
